{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50c2f80-7ace-4156-936f-394fe1d5374e",
   "metadata": {
    "id": "f50c2f80-7ace-4156-936f-394fe1d5374e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '15'\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "import string\n",
    "from keras.metrics import CategoricalAccuracy, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd5f672-797f-49e3-9535-fde2ce8e1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '6-persian-topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aeee35-336f-46ba-b24c-af2ea218bd4c",
   "metadata": {
    "id": "90aeee35-336f-46ba-b24c-af2ea218bd4c"
   },
   "outputs": [],
   "source": [
    "num = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539f51a8-8506-450e-88b9-2d797278e4e7",
   "metadata": {
    "id": "539f51a8-8506-450e-88b9-2d797278e4e7"
   },
   "outputs": [],
   "source": [
    "data_features = []\n",
    "data_targets = []\n",
    "for n in num:\n",
    "    class_path = os.path.join(base_dir , n)\n",
    "    file_path = os.listdir(class_path)\n",
    "    for p in file_path:\n",
    "        matn = os.path.join(class_path , p)\n",
    "        file = open(matn , 'r')\n",
    "        file_r = file.read()\n",
    "        if len(file_r) < 20:\n",
    "            os.remove(matn)\n",
    "        else:\n",
    "            data_features.append(matn)\n",
    "            data_targets.append(num.index(n))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae077d49-5abc-42c0-9f9c-b29a417487a7",
   "metadata": {
    "id": "ae077d49-5abc-42c0-9f9c-b29a417487a7"
   },
   "outputs": [],
   "source": [
    "train_f , val_f , train_t , val_t = train_test_split(data_features , data_targets , random_state=42 , test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dIEYHXMtUWf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIEYHXMtUWf5",
    "outputId": "5e6282b7-3440-44ff-e33c-5761340b142a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37253"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5421cf1-26c9-4257-9f54-a68c5fddd3cd",
   "metadata": {
    "id": "f5421cf1-26c9-4257-9f54-a68c5fddd3cd"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  stripped_ye = tf.strings.regex_replace(stripped_html, 'ي', 'ی')\n",
    "  stripped_ke = tf.strings.regex_replace(stripped_ye, 'ك', 'ک')\n",
    "  stripped_alef = tf.strings.regex_replace(stripped_ke, 'آ', 'ا')\n",
    "  stripped_english = tf.strings.regex_replace(stripped_alef, '[a-zA-Z]', ' ')\n",
    "  return tf.strings.regex_replace(stripped_alef,\n",
    "                                  '[%s]' % re.escape(string.punctuation),'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d03eb4-813c-45e8-ab72-149ec5bab592",
   "metadata": {
    "id": "24d03eb4-813c-45e8-ab72-149ec5bab592"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:06:59.237153: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.258064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.258267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.259587: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.259722: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.259803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.302200: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.302401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.302503: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:06:59.302574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1730 MB memory:  -> device: 0, name: NVIDIA GeForce MX330, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qQV02fTp2MNH",
   "metadata": {
    "id": "qQV02fTp2MNH"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_text(path, lbl):\n",
    "    lbl = tf.one_hot(lbl, depth=6, dtype='float32')\n",
    "    text = tf.io.read_file(path)\n",
    "    return text, lbl\n",
    "\n",
    "def vectorize_text(text, lbl):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, lbl\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_f, train_t))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_f, val_t))\n",
    "\n",
    "train_ds = train_ds.map(load_text)\n",
    "vectorize_layer.adapt(train_ds.map(lambda text, label: text))\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "\n",
    "val_ds = val_ds.map(load_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.shuffle(1000).batch(128)\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(128).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4rqN8dpeuXQ",
   "metadata": {
    "id": "d4rqN8dpeuXQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0xutrj_7excB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xutrj_7excB",
    "outputId": "7aa6687a-6b08-4e30-f18f-1bfce14cf7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               1700      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 162306 (634.01 KB)\n",
      "Trainable params: 162306 (634.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(100),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(6)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "PUkYgn63hkvm",
   "metadata": {
    "id": "PUkYgn63hkvm"
   },
   "outputs": [],
   "source": [
    "metrics = metrics = [\n",
    "    CategoricalAccuracy(name='accuracy'),\n",
    "    F1Score(name='f1-score')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Kz-qEb4nfKA8",
   "metadata": {
    "id": "Kz-qEb4nfKA8"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3_Z25iaYcH5o",
   "metadata": {
    "id": "3_Z25iaYcH5o"
   },
   "outputs": [],
   "source": [
    "train_iterator = train_ds.as_numpy_iterator()\n",
    "\n",
    "class_counts = {}\n",
    "class_indices = {}\n",
    "\n",
    "\n",
    "class_index = 0\n",
    "for data, labels in train_iterator:\n",
    "    for label in labels:\n",
    "        label_tuple = tuple(label)\n",
    "        if label_tuple not in class_counts:\n",
    "            class_counts[label_tuple] = 1\n",
    "            class_indices[label_tuple] = class_index\n",
    "            class_index += 1\n",
    "        else:\n",
    "            class_counts[label_tuple] += 1\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "\n",
    "class_weights = {\n",
    "    class_indices[label]: total_samples / count\n",
    "    for label, count in class_counts.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "I_mIOiVpzx4w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_mIOiVpzx4w",
    "outputId": "47fc5965-d209-4f38-91ed-af3d812ae9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:07:41.607302: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-10 12:07:42.197535: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f67b0cea3a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-10 12:07:42.197561: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce MX330, Compute Capability 6.1\n",
      "2024-05-10 12:07:42.201337: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-10 12:07:42.212299: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715357262.271048    5924 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 19s 74ms/step - loss: 9.0429 - accuracy: 0.7192 - f1-score: 0.4191 - val_loss: 0.3118 - val_accuracy: 0.9099 - val_f1-score: 0.5184\n",
      "Epoch 2/10\n",
      " 63/233 [=======>......................] - ETA: 0s - loss: 2.2095 - accuracy: 0.9158 - f1-score: 0.5281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:07:59.798828: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 1.8396 - accuracy: 0.9201 - f1-score: 0.5445 - val_loss: 0.2248 - val_accuracy: 0.9260 - val_f1-score: 0.5838\n",
      "Epoch 3/10\n",
      " 66/233 [=======>......................] - ETA: 0s - loss: 1.3978 - accuracy: 0.9347 - f1-score: 0.6227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:00.634762: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.9688 - accuracy: 0.9511 - f1-score: 0.7212 - val_loss: 0.1696 - val_accuracy: 0.9518 - val_f1-score: 0.7296\n",
      "Epoch 4/10\n",
      " 66/233 [=======>......................] - ETA: 0s - loss: 0.6096 - accuracy: 0.9721 - f1-score: 0.8289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:01.477522: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.4996 - accuracy: 0.9779 - f1-score: 0.8759 - val_loss: 0.1157 - val_accuracy: 0.9701 - val_f1-score: 0.8507\n",
      "Epoch 5/10\n",
      " 67/233 [=======>......................] - ETA: 0s - loss: 0.3764 - accuracy: 0.9855 - f1-score: 0.9307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:02.321517: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.9860 - f1-score: 0.9333 - val_loss: 0.1266 - val_accuracy: 0.9719 - val_f1-score: 0.8549\n",
      "Epoch 6/10\n",
      " 66/233 [=======>......................] - ETA: 0s - loss: 0.2080 - accuracy: 0.9914 - f1-score: 0.9542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:03.162359: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.2757 - accuracy: 0.9891 - f1-score: 0.9478 - val_loss: 0.1226 - val_accuracy: 0.9742 - val_f1-score: 0.8723\n",
      "Epoch 7/10\n",
      " 65/233 [=======>......................] - ETA: 0s - loss: 0.1287 - accuracy: 0.9945 - f1-score: 0.9724"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:04.024716: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.3006 - accuracy: 0.9894 - f1-score: 0.9493 - val_loss: 0.1275 - val_accuracy: 0.9732 - val_f1-score: 0.8538\n",
      "Epoch 8/10\n",
      " 66/233 [=======>......................] - ETA: 0s - loss: 0.1082 - accuracy: 0.9953 - f1-score: 0.9748"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:04.918740: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.3028 - accuracy: 0.9891 - f1-score: 0.9514 - val_loss: 0.1456 - val_accuracy: 0.9714 - val_f1-score: 0.8519\n",
      "Epoch 9/10\n",
      " 60/233 [======>.......................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9961 - f1-score: 0.9803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:05.804500: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.2020 - accuracy: 0.9936 - f1-score: 0.9724 - val_loss: 0.1514 - val_accuracy: 0.9745 - val_f1-score: 0.8887\n",
      "Epoch 10/10\n",
      " 66/233 [=======>......................] - ETA: 0s - loss: 0.1400 - accuracy: 0.9959 - f1-score: 0.9860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:06.729117: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 1s 4ms/step - loss: 0.2341 - accuracy: 0.9932 - f1-score: 0.9739 - val_loss: 0.1520 - val_accuracy: 0.9755 - val_f1-score: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:08:07.596962: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=10,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30,\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "u4qH2PSK6hYS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4qH2PSK6hYS",
    "outputId": "6328557a-def5-4ad8-8555-89bf3870c06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 1s 9ms/step - loss: 0.1571 - accuracy: 0.9741 - f1-score: 0.8875\n",
      "Evaluation result: [0.15711252391338348, 0.9740974307060242, array([0.92470276, 0.7432432 , 0.9912179 , 0.9930527 , 0.7167235 ,\n",
      "       0.95582026], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(val_ds)\n",
    "print('Evaluation result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95lJaQK5ZXG1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95lJaQK5ZXG1",
    "outputId": "77f04869-c1be-4500-ce61-ef62072c7e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 702us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_ds)\n",
    "predicted_classes = tf.math.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Npe7gmc3ZwRn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Npe7gmc3ZwRn",
    "outputId": "7cc84bf6-e257-4a9b-b561-4284059c569b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       393\n",
      "           1       0.73      0.75      0.74        73\n",
      "           2       0.99      0.99      0.99      4339\n",
      "           3       0.99      0.99      0.99      1224\n",
      "           4       0.58      0.93      0.72       113\n",
      "           5       0.96      0.95      0.96      1309\n",
      "\n",
      "    accuracy                           0.97      7451\n",
      "   macro avg       0.87      0.92      0.89      7451\n",
      "weighted avg       0.98      0.97      0.98      7451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(val_t, predicted_classes)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8d1ca-8b9e-4500-8ae9-6c486b832c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50c2f80-7ace-4156-936f-394fe1d5374e",
   "metadata": {
    "id": "f50c2f80-7ace-4156-936f-394fe1d5374e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:09:18.769550: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-10 12:09:18.796727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 12:09:18.796754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 12:09:18.797408: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-10 12:09:18.802093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 12:09:19.316557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '15'\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "import string\n",
    "from keras.metrics import CategoricalAccuracy, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915748e-8f43-404a-b0cf-59d4a455eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '6-persian-topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aeee35-336f-46ba-b24c-af2ea218bd4c",
   "metadata": {
    "id": "90aeee35-336f-46ba-b24c-af2ea218bd4c"
   },
   "outputs": [],
   "source": [
    "num = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539f51a8-8506-450e-88b9-2d797278e4e7",
   "metadata": {
    "id": "539f51a8-8506-450e-88b9-2d797278e4e7"
   },
   "outputs": [],
   "source": [
    "data_features = []\n",
    "data_targets = []\n",
    "for n in num:\n",
    "    class_path = os.path.join(base_dir , n)\n",
    "    file_path = os.listdir(class_path)\n",
    "    for p in file_path:\n",
    "        matn = os.path.join(class_path , p)\n",
    "        file = open(matn , 'r')\n",
    "        file_r = file.read()\n",
    "        if len(file_r) < 20:\n",
    "            os.remove(matn)\n",
    "        else:\n",
    "            data_features.append(matn)\n",
    "            data_targets.append(num.index(n))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae077d49-5abc-42c0-9f9c-b29a417487a7",
   "metadata": {
    "id": "ae077d49-5abc-42c0-9f9c-b29a417487a7"
   },
   "outputs": [],
   "source": [
    "train_f , val_f , train_t , val_t = train_test_split(data_features , data_targets , random_state=42 , test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dIEYHXMtUWf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIEYHXMtUWf5",
    "outputId": "5e6282b7-3440-44ff-e33c-5761340b142a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37253"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5421cf1-26c9-4257-9f54-a68c5fddd3cd",
   "metadata": {
    "id": "f5421cf1-26c9-4257-9f54-a68c5fddd3cd"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  stripped_ye = tf.strings.regex_replace(stripped_html, 'ي', 'ی')\n",
    "  stripped_ke = tf.strings.regex_replace(stripped_ye, 'ك', 'ک')\n",
    "  stripped_alef = tf.strings.regex_replace(stripped_ke, 'آ', 'ا')\n",
    "  stripped_english = tf.strings.regex_replace(stripped_alef, '[a-zA-Z]', ' ')\n",
    "  return tf.strings.regex_replace(stripped_alef,\n",
    "                                  '[%s]' % re.escape(string.punctuation),'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d03eb4-813c-45e8-ab72-149ec5bab592",
   "metadata": {
    "id": "24d03eb4-813c-45e8-ab72-149ec5bab592"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:09:23.471842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.492137: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.492313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.493946: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.494110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.494224: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.532573: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.532690: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.532770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-10 12:09:23.532836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1730 MB memory:  -> device: 0, name: NVIDIA GeForce MX330, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qQV02fTp2MNH",
   "metadata": {
    "id": "qQV02fTp2MNH"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_text(path, lbl):\n",
    "    lbl = tf.one_hot(lbl, depth=6, dtype='float32')\n",
    "    text = tf.io.read_file(path)\n",
    "    return text, lbl\n",
    "\n",
    "def vectorize_text(text, lbl):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, lbl\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_f, train_t))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_f, val_t))\n",
    "\n",
    "train_ds = train_ds.map(load_text)\n",
    "vectorize_layer.adapt(train_ds.map(lambda text, label: text))\n",
    "train_ds = train_ds.map(vectorize_text)\n",
    "\n",
    "val_ds = val_ds.map(load_text)\n",
    "val_ds = val_ds.map(vectorize_text)\n",
    "\n",
    "train_ds = train_ds.shuffle(1000).batch(128)\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(128).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4rqN8dpeuXQ",
   "metadata": {
    "id": "d4rqN8dpeuXQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0xutrj_7excB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xutrj_7excB",
    "outputId": "b0f9072e-2461-41a9-9f1d-cf13247c2e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:10:18.012514: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160000    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 32)                4224      \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               3300      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168130 (656.76 KB)\n",
      "Trainable params: 168130 (656.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features, embedding_dim),\n",
    "  layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "  layers.Dense(100),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(6)])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "PUkYgn63hkvm",
   "metadata": {
    "id": "PUkYgn63hkvm"
   },
   "outputs": [],
   "source": [
    "metrics = metrics = [\n",
    "    CategoricalAccuracy(name='accuracy'),\n",
    "    F1Score(name='f1-score')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Kz-qEb4nfKA8",
   "metadata": {
    "id": "Kz-qEb4nfKA8"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3_Z25iaYcH5o",
   "metadata": {
    "id": "3_Z25iaYcH5o"
   },
   "outputs": [],
   "source": [
    "train_iterator = train_ds.as_numpy_iterator()\n",
    "\n",
    "class_counts = {}\n",
    "class_indices = {}\n",
    "\n",
    "\n",
    "class_index = 0\n",
    "for data, labels in train_iterator:\n",
    "    for label in labels:\n",
    "        label_tuple = tuple(label)\n",
    "        if label_tuple not in class_counts:\n",
    "            class_counts[label_tuple] = 1\n",
    "            class_indices[label_tuple] = class_index\n",
    "            class_index += 1\n",
    "        else:\n",
    "            class_counts[label_tuple] += 1\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "\n",
    "class_weights = {\n",
    "    class_indices[label]: total_samples / count\n",
    "    for label, count in class_counts.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "I_mIOiVpzx4w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_mIOiVpzx4w",
    "outputId": "6fdcb89e-1689-4aa1-e5e7-0d3e229e0ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:10:26.881411: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-05-10 12:10:27.607521: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f2f10afd0d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-10 12:10:27.607548: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce MX330, Compute Capability 6.1\n",
      "2024-05-10 12:10:27.611869: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715357427.676116    8112 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 23s 88ms/step - loss: 5.7700 - accuracy: 0.8908 - f1-score: 0.5673 - val_loss: 0.1887 - val_accuracy: 0.9529 - val_f1-score: 0.7158\n",
      "Epoch 2/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 1.8822 - accuracy: 0.9598 - f1-score: 0.7350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:10:48.588463: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 1.1940 - accuracy: 0.9719 - f1-score: 0.7652 - val_loss: 0.1562 - val_accuracy: 0.9594 - val_f1-score: 0.7502\n",
      "Epoch 3/10\n",
      "  6/233 [..............................] - ETA: 5s - loss: 0.7537 - accuracy: 0.9792 - f1-score: 0.8048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:10:54.611985: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.4799 - accuracy: 0.9846 - f1-score: 0.8493 - val_loss: 0.1153 - val_accuracy: 0.9747 - val_f1-score: 0.8549\n",
      "Epoch 4/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.1667 - accuracy: 0.9911 - f1-score: 0.9174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:00.662731: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.4175 - accuracy: 0.9877 - f1-score: 0.9023 - val_loss: 0.1245 - val_accuracy: 0.9768 - val_f1-score: 0.8843\n",
      "Epoch 5/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.1692 - accuracy: 0.9922 - f1-score: 0.9225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:06.702237: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.5044 - accuracy: 0.9890 - f1-score: 0.9230 - val_loss: 0.1473 - val_accuracy: 0.9732 - val_f1-score: 0.8682\n",
      "Epoch 6/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.1534 - accuracy: 0.9944 - f1-score: 0.9615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:12.784505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.4378 - accuracy: 0.9898 - f1-score: 0.9185 - val_loss: 0.1318 - val_accuracy: 0.9734 - val_f1-score: 0.8664\n",
      "Epoch 7/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.2633 - accuracy: 0.9900 - f1-score: 0.9261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:18.909896: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.3211 - accuracy: 0.9925 - f1-score: 0.9469 - val_loss: 0.1343 - val_accuracy: 0.9698 - val_f1-score: 0.8698\n",
      "Epoch 8/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.3610 - accuracy: 0.9944 - f1-score: 0.9599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:25.071365: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.1812 - accuracy: 0.9948 - f1-score: 0.9626 - val_loss: 0.1430 - val_accuracy: 0.9737 - val_f1-score: 0.8741\n",
      "Epoch 9/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.0743 - accuracy: 0.9978 - f1-score: 0.9833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:31.231323: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.0901 - accuracy: 0.9970 - f1-score: 0.9786 - val_loss: 0.1362 - val_accuracy: 0.9760 - val_f1-score: 0.8823\n",
      "Epoch 10/10\n",
      "  7/233 [..............................] - ETA: 5s - loss: 0.0669 - accuracy: 0.9978 - f1-score: 0.9874"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:37.347461: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 6s 26ms/step - loss: 0.0673 - accuracy: 0.9979 - f1-score: 0.9873 - val_loss: 0.1379 - val_accuracy: 0.9758 - val_f1-score: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:11:43.430508: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=10,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30,\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "u4qH2PSK6hYS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4qH2PSK6hYS",
    "outputId": "5496d928-6a25-41bd-fb16-2d9bb79fcb8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 1s 10ms/step - loss: 0.1431 - accuracy: 0.9768 - f1-score: 0.8921\n",
      "Evaluation result: [0.1430804431438446, 0.9767816662788391, array([0.9258778 , 0.72868216, 0.9920608 , 0.98771495, 0.7539683 ,\n",
      "       0.96449023], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(val_ds)\n",
    "print('Evaluation result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95lJaQK5ZXG1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95lJaQK5ZXG1",
    "outputId": "f64b056f-4268-4855-abd3-16e3bbae39c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 1s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_ds)\n",
    "predicted_classes = tf.math.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Npe7gmc3ZwRn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Npe7gmc3ZwRn",
    "outputId": "2c005dda-5b26-4e89-bcd8-cc9ebda04915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       393\n",
      "           1       0.84      0.64      0.73        73\n",
      "           2       0.99      0.99      0.99      4339\n",
      "           3       0.99      0.99      0.99      1224\n",
      "           4       0.68      0.84      0.75       113\n",
      "           5       0.96      0.96      0.96      1309\n",
      "\n",
      "    accuracy                           0.98      7451\n",
      "   macro avg       0.90      0.89      0.89      7451\n",
      "weighted avg       0.98      0.98      0.98      7451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(val_t, predicted_classes)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
